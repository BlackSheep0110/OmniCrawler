OmniCrawler ğŸ•·ï¸

OmniCrawler is a robust, modular, and general-purpose web scraping framework designed to build datasets from the web. Whether you are researching AI, Real Estate, Finance, or Medical data, OmniCrawler helps you discover relevant domains and bulk-download articles.

Developed by: Ahmad Salami Far

It operates in an Auto-Pilot mode:

Discovery: Scans ANY input file (Text, Word, Logs, etc.) for links, filters relevant domains, and maps articles.

Extraction: Automatically switches to multithreaded downloading of clean text content.

OmniCrawler (Ø®Ø²Ø´Ú¯Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙˆØ¨)

OmniCrawler ÛŒÚ© ÙØ±ÛŒÙ…â€ŒÙˆØ±Ú© Ù…ØªÙ†â€ŒØ¨Ø§Ø² Ùˆ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² ÙˆØ¨ Ø§Ø³Øª. Ø§ÛŒÙ† Ø§Ø¨Ø²Ø§Ø± Ø¨Ù‡ Ù‡ÛŒÚ† Ù…ÙˆØ¶ÙˆØ¹ Ø®Ø§ØµÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù†ÛŒØ³Øª Ùˆ ØªÙˆØ³Ø· Ø§Ø­Ù…Ø¯ Ø³Ù„Ø§Ù…ÛŒâ€ŒÙØ± ØªÙˆØ³Ø¹Ù‡ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª.

âœ¨ Key Features / ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ

ğŸ”„ Auto-Pilot Mode: Automatically runs discovery and then switches to download mode without user intervention.

Ø­Ø§Ù„Øª Ø®Ù„Ø¨Ø§Ù† Ø®ÙˆØ¯Ú©Ø§Ø±: Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø´Øªâ€ŒØ³Ø±Ù‡Ù… ÙØ§Ø² Ú©Ø´Ù Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø®Ø§Ù„Øª Ú©Ø§Ø±Ø¨Ø±.

ğŸ’¾ Smart Resume & Checkpoint: Saves found links instantly. If the internet cuts off or the app crashes, you can resume exactly where you left off.

Ø°Ø®ÛŒØ±Ù‡ Ù„Ø­Ø¸Ù‡â€ŒØ§ÛŒ Ùˆ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¯Ø§Ù…Ù‡: Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ù„Ø§ÙØ§ØµÙ„Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø§Ú¯Ø± Ø¨Ø±Ù‚ Ø¨Ø±ÙˆØ¯ ÛŒØ§ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø³ØªÙ‡ Ø´ÙˆØ¯ØŒ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ Ø§Ø² Ù‡Ù…Ø§Ù†â€ŒØ¬Ø§ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

ğŸ“‚ Universal Input Reader: Extracts links from dirty/messy text files, logs, code, or Word documents (.docx).

ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø§Ù…Ø­Ø¯ÙˆØ¯: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø´Ù„ÙˆØºØŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±Ø¯ Ùˆ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ.

ğŸ“ File Logging: Keeps a detailed history of operations in scraper_report.log.

Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ ÙØ§ÛŒÙ„: Ø«Ø¨Øª Ø¯Ù‚ÛŒÙ‚ ØªÙ…Ø§Ù… ÙˆÙ‚Ø§ÛŒØ¹ Ùˆ Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ.

Topic Agnostic: Fully customizable via config.py.

Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù…ÙˆØ¶ÙˆØ¹: ØªÙ†Ù‡Ø§ Ø¨Ø§ ØªØºÛŒÛŒØ± Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ù‡Ø¯Ù Ø±Ø¨Ø§Øª Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹Â  Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ø¨ÙˆØ±Ø³).

ğŸš€ Quick Start / Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

1. Installation (Ù†ØµØ¨)

git clone [https://github.com/ahmadsalamifar/OmniCrawler.git](https://github.com/ahmadsalamifar/OmniCrawler.git)
cd OmniCrawler
pip install -r requirements.txt


2. Configuration (ØªÙ†Ø¸ÛŒÙ…Ø§Øª)

Open config.py and set TARGET_KEYWORDS.
ÙØ§ÛŒÙ„ config.py Ø±Ø§ Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯ Ùˆ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù‡Ø¯Ù Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹ "Ú©Ø§Ù†Ú©Ø³"ØŒ "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ").

3. Usage (Ù†Ø­ÙˆÙ‡ Ø§Ø¬Ø±Ø§)

Just drop your input files (files containing links) into the folder and run:
ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø­Ø§ÙˆÛŒ Ù„ÛŒÙ†Ú© (Ù‡Ø± ÙØ§ÛŒÙ„ÛŒ Ø¨Ø§ Ù‡Ø± Ù¾Ø³ÙˆÙ†Ø¯ÛŒ) Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø¨Ø±ÛŒØ²ÛŒØ¯ Ùˆ ÙÙ‚Ø· Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:

python main.py


The script will find links, filter them, save the queue, and automatically start downloading.

Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø±Ø§ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Manual Modes (Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÛŒ - Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

If you want to run specific phases manually:
Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ ÙØ§Ø²Ù‡Ø§ Ø±Ø§ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:

Only Find Links (ÙÙ‚Ø· Ø¬Ø³ØªØ¬Ùˆ):

python main.py --mode discovery


Only Download (ÙÙ‚Ø· Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² Ù„ÛŒØ³Øª Ø¢Ù…Ø§Ø¯Ù‡):

python main.py --mode download


ğŸ“‚ Project Structure / Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡

config.py: Settings (Keywords, threads, timeouts).

main.py: The brain of the scraper (Auto-switching logic).

scraper_report.log: Detailed logs of what happened.

download_queue.txt: The queue of found links (Auto-saved).

Scraped_Data/: Where downloaded articles are saved.

Note: Built for educational purposes. Please respect robots.txt.